# EKS NVIDIA P5 (H100) on EKS 1.23

This example is specific to EKS 1.23 which still utilizes the Docker container runtime (replaced with containerd starting on EKS 1.24+). The gpu-operator defaults to using the Docker runtime, so this example is provided to demonstrate how to deploy the gpu-operator on EKS 1.23.

## Prerequisites:

Ensure that you have the following tools installed locally:

1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

Before you begin, you will need to locate an availability zone that meets your instance selection requirements. This example utilizes placement groups to cluster instances in a single availability zone to reduce network latency. To find an availability zone that supports the instance types you wish to use, run the following command, replacing the region with your region of choice:

```sh
aws ec2 describe-instance-type-offerings --location-type availability-zone  \
      --filters Name=instance-type,Values=p5.48xlarge \
      --region <REGION> --output table
```

In addition, you will need to create an AMI that is suitable for the P5 instance type. To create an AMI for this example, clone this project on your local machine https://github.com/clowdhaus/amazon-eks-gpu-ami and follow its README for building an AMI. The command used to build the AMI for this example is:

```sh
packer build -var-file=gpu.pkrvars.hcl -var "eks_version=1.23" .
```

The AMI ID generated by that build process should be placed in the local variable for `ami_id` in the `eks.tf` file (top of file) *BEFORE deploying the example.

## Deploy

To provision this example:

```sh
terraform init
terraform apply
```

Enter `yes` at command prompt to apply

## Validate

1. Run `update-kubeconfig` command, using the Terraform provided Output, replace with your `$AWS_REGION` and your `$CLUSTER_NAME` variables.

```sh
aws eks --region <$AWS_REGION> update-kubeconfig --name <$CLUSTER_NAME>
```

2. View the logs of the `cuda-validation` container in the `nvidia-cuda-validator` pod deployed by the gpu-operator:

```sh
kubectl logs nvidia-cuda-validator-t2mrd -n gpu-operator -c cuda-validation

[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
```

3. Verify the output of the `nvidia-cuda-validator` container in the `nvidia-cuda-validator` pod deployed by the gpu-operator:

```sh
kubectl logs nvidia-cuda-validator-t2mrd -n gpu-operator -c nvidia-cuda-validator
cuda workload validation is successful
```

4. Deploy the `nvidia-smi.yaml` pod and view its output:

```sh
kubectl apply -f nvidia-smi.yaml
kubectl logs gpu-test-nvidia-smi

Fri Sep  1 15:03:31 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:A8:00.0 Off |                    0 |
| N/A   24C    P0              70W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

5. Deploy the `vectoradd.yaml` pod and view its output:

```sh
kubectl apply -f vectoradd.yaml
kubectl logs gpu-test-vectoradd

[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
```

## Destroy

To teardown and remove the resources created in this example:

```sh
terraform destroy -target module.eks_blueprints_addons -auto-approve
terraform destroy -target module.eks -auto-approve
terraform destroy -auto-approve
```
